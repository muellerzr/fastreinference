{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure\n",
    "\n",
    "> The foundational event system for `Performer` based on fastai `Callback`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from abc import ABC, abstractmethod, abstractproperty\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from fastreinference.utils import SelfEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@SelfEnum\n",
    "class DeviceType:\n",
    "    \"\"\"\n",
    "    Enum of all supported device placements\n",
    "    \"\"\"\n",
    "    CPU:Any\n",
    "    CUDA:Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_default_device():\n",
    "    \"\"\"\n",
    "    Returns `DeviceType.CPU` if GPU is not available, else `DeviceType.CUDA`\n",
    "    \"\"\"\n",
    "    return DeviceType.CPU if not torch.cuda.is_available() else DeviceType.CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@SelfEnum(special=[\"INFERENCE\"])\n",
    "class ManagerType:\n",
    "    \"\"\"\n",
    "    Enum of the various context manager options you can use when doing inference, with documentation of its members\n",
    "    \"\"\"\n",
    "    NO_GRAD:Any = \"Run with `torch.no_grad`\"\n",
    "    INFERENCE = \"inference_mode\", \"Run with `torch.inference_mode`\"\n",
    "    NONE:Any = \"Keep all gradients and apply no context managers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"no_grad\" class=\"doc_header\"><code>no_grad</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "Run with `torch.no_grad`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ManagerType.NO_GRAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"inference_mode\" class=\"doc_header\"><code>inference_mode</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "Run with `torch.inference_mode`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ManagerType.INFERENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"none\" class=\"doc_header\"><code>none</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "Keep all gradients and apply no context managers"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ManagerType.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InferenceConfiguration(ABC):\n",
    "    \"\"\"\n",
    "    The foundational class for customizing behaviors during inference.\n",
    "    \n",
    "    There are three methods available that must be implemented:\n",
    "      - `after_drawn_batch`\n",
    "      - `gather_predictions`\n",
    "      - `decoding_values`\n",
    "      \n",
    "    If an implementation should stay to its default behavior, do the following:\n",
    "      - `event_name(self, *args): return super().event_name(*args)`\n",
    "      \n",
    "    Where `event_name` is any of the three events listed above\n",
    "    \n",
    "    A `context` can be set with a `ManagerType` for what type of context manager should be ran at inference time\n",
    "    \"\"\"\n",
    "    context: ManagerType = ManagerType.NO_GRAD # Context manager to be ran at inference time\n",
    "    device: DeviceType = get_default_device() # Device to be used during inference. Default is cuda if available\n",
    "    \n",
    "    @abstractmethod\n",
    "    def after_drawn_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Called immediatly after a batch has been drawn from the `DataLoader`.\n",
    "        Any final adjustments to the batch before being sent to the model should be done here.\n",
    "        \n",
    "        Default implementation is to return `batch`.\n",
    "        \"\"\"\n",
    "        return batch\n",
    "    \n",
    "    @abstractmethod\n",
    "    def gather_predictions(self, model, batch): \n",
    "        \"\"\"\n",
    "        Performs inference with `model` on `batch`.\n",
    "        Any specific inference decorators such as `no_grad` or `inference_mode` is done in `Performer`.\n",
    "        \n",
    "        Default implementation is `model(*batch)`.\n",
    "        \"\"\"\n",
    "        return model(*batch)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def decoding_values(self, values): \n",
    "        \"\"\"\n",
    "        Called after predictions have been gathered on a `batch`.\n",
    "        Any specific class decoding and final datatype preparation should be done here.\n",
    "        \n",
    "        Default implementation is to return `values`.\n",
    "        \"\"\"\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"InferenceConfiguration\" class=\"doc_header\"><code>class</code> <code>InferenceConfiguration</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>InferenceConfiguration</code>() :: `ABC`\n",
       "\n",
       "The foundational class for customizing behaviors during inference.\n",
       "\n",
       "There are three methods available that must be implemented:\n",
       "  - `after_drawn_batch`\n",
       "  - `gather_predictions`\n",
       "  - `decoding_values`\n",
       "  \n",
       "If an implementation should stay to its default behavior, do the following:\n",
       "  - `event_name(self, *args): return super().event_name(*args)`\n",
       "  \n",
       "Where `event_name` is any of the three events listed above\n",
       "\n",
       "A `context` can also be set with a `ManagerType` for what type of context manager should be ran at inference time"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(InferenceConfiguration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"InferenceConfiguration.gather_predictions\" class=\"doc_header\"><code>InferenceConfiguration.gather_predictions</code><a href=\"__main__.py#L30\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>InferenceConfiguration.gather_predictions</code>(**`model`**, **`batch`**)\n",
       "\n",
       "Performs inference with `model` on `batch`.\n",
       "Any specific inference decorators such as `no_grad` or `inference_mode` is done in `Performer`.\n",
       "\n",
       "Default implementation is `model(*batch)`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(InferenceConfiguration.gather_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why abstract: Force users to think about if this is how they want their code in prod to be ran\n",
    "# Since there would only be one level, easy to track where and how"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifierConfiguration(InferenceConfiguration):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "    def after_drawn_batch(self, batch): super().after_drawn_batch(batch)\n",
    "    def gather_predictions(self, model, batch):\n",
    "        return model(*batch)\n",
    "    def decoding_values(self, values):\n",
    "        preds = values.argmax(dim=-1)\n",
    "        decoded_preds = [self.vocab[p] for p in preds]\n",
    "        return {\"classes\":decoded_preds, \"probabilities\":preds}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
