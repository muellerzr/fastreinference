# AUTOGENERATED! DO NOT EDIT! File to edit: 01_configure.ipynb (unless otherwise specified).

__all__ = ['DeviceType', 'get_default_device', 'ManagerType', 'InferenceConfiguration']

# Cell
from abc import ABC, abstractmethod, abstractproperty

# import torch
from .typing import enumify, Member

# Cell
@enumify
class DeviceType:
    """
    Enum of all supported device placements
    """
    CPU:Member
    CUDA:Member

# Cell
def get_default_device():
    """
    Returns `DeviceType.CPU` if GPU is not available, else `DeviceType.CUDA`
    """
    return DeviceType.CPU if not torch.cuda.is_available() else DeviceType.CUDA

# Cell
@enumify
class ManagerType:
    """
    Enum of the various context manager options you can use when doing inference, with documentation of its members
    """
    NO_GRAD:Member["Run with `torch.no_grad`"]
    INFERENCE:Member["inference_mode", "Run with `torch.inference_mode`"]
    NONE:Member["Keep all gradients and apply no context managers"]

# Cell
class InferenceConfiguration(ABC):
    """
    The foundational class for customizing behaviors during inference.

    There are three methods available that must be implemented:
      - `after_drawn_batch`
      - `gather_predictions`
      - `decoding_values`

    If an implementation should stay to its default behavior, do the following:
      - `event_name(self, *args): return super().event_name(*args)`

    Where `event_name` is any of the three events listed above

    A `context` can be set with a `ManagerType` for what type of context manager should be ran at inference time
    """
    context: ManagerType = ManagerType.NO_GRAD # Context manager to be ran at inference time
    device: DeviceType = get_default_device() # Device to be used during inference. Default is cuda if available

    @abstractmethod
    def after_drawn_batch(self, batch):
        """
        Called immediatly after a batch has been drawn from the `DataLoader`.
        Any final adjustments to the batch before being sent to the model should be done here.

        Default implementation is to return `batch`.
        """
        return batch

    @abstractmethod
    def gather_predictions(self, model, batch):
        """
        Performs inference with `model` on `batch`.
        Any specific inference decorators such as `no_grad` or `inference_mode` is done in `Performer`.

        Default implementation is `model(*batch)`.
        """
        return model(*batch)

    @abstractmethod
    def decoding_values(self, values):
        """
        Called after predictions have been gathered on a `batch`.
        Any specific class decoding and final datatype preparation should be done here.

        Default implementation is to return `values`.
        """
        return values